{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import t, shapiro, kstest\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation\n",
    "def prepare_data():\n",
    "    #loading data\n",
    "    abundance = pd.read_csv('~/icr/simko/data/simko2_data/passport_prots.csv', index_col=0)\n",
    "    abundance.index = abundance.index.astype(str)\n",
    "\n",
    "    #removing cell lines with over 4000 nans\n",
    "    nans_per_cl = abundance.isna().sum(axis=0)\n",
    "    abundance_cl_filtered = abundance.loc[:, nans_per_cl<4000]\n",
    "\n",
    "    #getting rid of protein with over 80% NaN (from the dataset filtered by CLs)\n",
    "    prot_nan_count = abundance_cl_filtered.isna().sum(axis=1)\n",
    "    prot_nan_percent = (prot_nan_count/abundance_cl_filtered.shape[1])*100\n",
    "\n",
    "    abundance_filtered = abundance_cl_filtered[prot_nan_percent<80]\n",
    "\n",
    "    #imputing witht the lower quartile average for each protein\n",
    "    #set the protein names as the index - ignores it while we find the lower quartile\n",
    "\n",
    "    def average_lower_quartile(x):\n",
    "        sorted_abundances = x.dropna().sort_values()\n",
    "        lower_qt_values = sorted_abundances.iloc[:int(len(sorted_abundances) * 0.25)]\n",
    "        return lower_qt_values.mean()\n",
    "\n",
    "\n",
    "    lower_qt_averages = abundance_filtered.apply(average_lower_quartile, axis=1)\n",
    "\n",
    "    abundance_filtered_no_nan = abundance_filtered.apply(lambda x: x.fillna(lower_qt_averages[x.name]), axis=1)\n",
    "\n",
    "    #transposing\n",
    "    abundance_imputed = abundance_filtered_no_nan.T\n",
    "\n",
    "    #scaling the imputed data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(abundance_imputed), index=abundance_imputed.index, columns=abundance_imputed.columns)\n",
    "\n",
    "    return scaled_data\n",
    "\n",
    "scaled_data = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definsing autoencoder\n",
    "# Define the Conditional VAE\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, input_dim, cond_dim, hidden_dim, latent_dim):\n",
    "        \"\"\"\n",
    "        input_dim: Number of protein features.\n",
    "        cond_dim: Dimensionality of the condition vector (e.g., same as input_dim if you use a binary mask).\n",
    "        hidden_dim: Number of hidden units.\n",
    "        latent_dim: Size of the latent space.\n",
    "        \"\"\"\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        \n",
    "        # Encoder: input is concatenation of data and condition.\n",
    "        self.fc1 = nn.Linear(input_dim + cond_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder: latent vector concatenated with condition.\n",
    "        self.fc3 = nn.Linear(latent_dim + cond_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "    \n",
    "    def encode(self, x, c):\n",
    "        # Concatenate the protein data and the condition vector.\n",
    "        x_cond = torch.cat([x, c], dim=1)\n",
    "        h = F.relu(self.fc1(x_cond))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z, c):\n",
    "        # Concatenate the latent code and the condition vector.\n",
    "        z_cond = torch.cat([z, c], dim=1)\n",
    "        h = F.relu(self.fc3(z_cond))\n",
    "        x_recon = self.fc4(h)\n",
    "        return x_recon\n",
    "    \n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z, c)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "# Loss function: Reconstruction loss (MSE for continuous data) + KL divergence\n",
    "def CVAE_loss_function(x_recon, x, mu, logvar):\n",
    "    # Reconstruction loss: Mean Squared Error summed over features.\n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction='sum')\n",
    "    # KL divergence loss\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Example: Synthetic Proteomics Data & Knockout Condition\n",
    "# ---------------------------\n",
    "\n",
    "#creating tensor\n",
    "X = scaled_data\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "#batch_size = 32\n",
    "#dataset = Data.TensorDataset(X_tensor)\n",
    "#dataloader = Data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#defining dimensions\n",
    "input_dim = X_tensor.shape[1]  # number of proteins\n",
    "cond_dim = X_tensor.shape[1]     # using a binary mask with same size (1 means active, 0 means knockout)\n",
    "hidden_dim = 32\n",
    "latent_dim = 10\n",
    "num_samples = 895\n",
    "\n",
    "# Create a condition vector: all ones initially\n",
    "c_data = torch.ones(X.shape[0], X.shape[1])  # Shape: (num_samples, input_dim)\n",
    "\n",
    "#selecting pbrm1 to be knocked out\n",
    "knockout_index = X.columns.get_loc('PBRM1')\n",
    "\n",
    "#setting pbrm1 to 0 in the condition data\n",
    "c_data[:, knockout_index] = 0\n",
    "\n",
    "# Create a dataset and DataLoader\n",
    "dataset = TensorDataset(X_tensor, c_data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Instantiate the model and optimizer\n",
    "model = ConditionalVAE(input_dim, cond_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 7045.9399\n",
      "Epoch 2/20, Loss: 6944.0444\n",
      "Epoch 3/20, Loss: 6932.0866\n",
      "Epoch 4/20, Loss: 6922.5169\n",
      "Epoch 5/20, Loss: 6915.3936\n",
      "Epoch 6/20, Loss: 6909.5964\n",
      "Epoch 7/20, Loss: 6905.4986\n",
      "Epoch 8/20, Loss: 6902.1624\n",
      "Epoch 9/20, Loss: 6899.7941\n",
      "Epoch 10/20, Loss: 6897.8827\n",
      "Epoch 11/20, Loss: 6896.6188\n",
      "Epoch 12/20, Loss: 6895.5571\n",
      "Epoch 13/20, Loss: 6894.7640\n",
      "Epoch 14/20, Loss: 6894.1056\n",
      "Epoch 15/20, Loss: 6893.8854\n",
      "Epoch 16/20, Loss: 6893.4927\n",
      "Epoch 17/20, Loss: 6893.2022\n",
      "Epoch 18/20, Loss: 6893.0658\n",
      "Epoch 19/20, Loss: 6892.9404\n",
      "Epoch 20/20, Loss: 6893.0669\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Training Loop\n",
    "# ---------------------------\n",
    "num_epochs = 20\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_c in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, mu, logvar = model(batch_x, batch_c)\n",
    "        loss = CVAE_loss_function(x_recon, batch_x, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataset):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Select a sample protein abundance profile\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     sample_x \u001b[38;5;241m=\u001b[39m x_data[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Create a condition vector that represents a knockout scenario.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# For instance, set the protein at index 10 to zero (knockout) while leaving others intact.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     sample_c \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m, cond_dim)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_data' is not defined"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Inference: Simulate a Knockout\n",
    "# ---------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Select a sample protein abundance profile\n",
    "    sample_x = x_data[0:1]\n",
    "    \n",
    "    # Create a condition vector that represents a knockout scenario.\n",
    "    # For instance, set the protein at index 10 to zero (knockout) while leaving others intact.\n",
    "    sample_c = torch.ones(1, cond_dim)\n",
    "    sample_c[0, 10] = 0  # knockout protein 10\n",
    "    \n",
    "    # Generate the reconstructed profile under the knockout condition.\n",
    "    x_recon, _, _ = model(sample_x, sample_c)\n",
    "    \n",
    "    print(\"Original protein profile (sample):\")\n",
    "    print(sample_x)\n",
    "    print(\"\\nReconstructed profile under knockout condition (protein 10 knocked out):\")\n",
    "    print(x_recon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
